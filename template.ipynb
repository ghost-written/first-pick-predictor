{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Pick Predictor\n",
    "\n",
    "**Name(s)**: Michael Kroyan\n",
    "\n",
    "**Website Link**: https://ghost-written.github.io/first-pick-predictor/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T05:23:03.908337Z",
     "start_time": "2025-06-06T05:23:03.902018Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "pd.options.plotting.backend = 'plotly'\n",
    "\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"browser\"\n",
    "\n",
    "from dsc80_utils import * # Feel free to uncomment and use this.\n",
    "from scipy.stats import binomtest\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import precision_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T05:23:04.234468Z",
     "start_time": "2025-06-06T05:23:04.229465Z"
    }
   },
   "outputs": [],
   "source": [
    "# I'm interested in:\n",
    "\n",
    "# Using team compositions to predict whether a team is likelier to win or lose.\n",
    "# Using team compositions to predict gold, XP, or kill diff at 25.\n",
    "# Using the champion pick list to predict the first ban.\n",
    "# Using the champion ban list to predict the first pick.\n",
    "\n",
    "# I think that I will use the champion ban list to predict the first pick. Banning comes before picking, chronologically speaking, \n",
    "# so the inverse would end up being in an invalid order. While the other predictions are interesting, I feel like the pick list depends \n",
    "# very directly on the ban list. Whether a team wins or loses is somewhat influenced by champions, but also by skill."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Data Cleaning and Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T05:23:07.516807Z",
     "start_time": "2025-06-06T05:23:04.282813Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read in the DataFrame we are going to use for this data analysis. \n",
    "\n",
    "df = pd.read_csv(\"2024_LoL_esports_match_data_from_OraclesElixir.csv\", low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T05:23:09.790342Z",
     "start_time": "2025-06-06T05:23:07.518815Z"
    }
   },
   "outputs": [],
   "source": [
    "# This is the data we will need for the question we intend to answer. \n",
    "\n",
    "# We are retaining gameid and participantid for pre-processing purposes. \n",
    "# We are going to move all data from both halves of the same game into each row and then drop the row for the team that picks second. \n",
    "# We are simply going to drop any rows with missingness. \n",
    "# For the purposes of our predictions, we need complete data, and there is no great way to impute data for these nominal categories.\n",
    "\n",
    "# For the purposes of most of my Exploratory Data Analysis, I will, however, refer back to the complete data set, from which we can draw \n",
    "# some interesting conclusions. We will come back to this version of the data set later.\n",
    "\n",
    "eval_df = pd.read_csv(\"2024_LoL_esports_match_data_from_OraclesElixir.csv\", low_memory=False)\n",
    "eval_df = eval_df[eval_df[\"participantid\"].isin([100, 200])]\n",
    "eval_df = eval_df[['gameid', 'participantid', 'ban1', 'ban2', 'ban3', 'ban4', 'ban5', 'pick1', 'patch', 'teamname']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T05:23:09.944912Z",
     "start_time": "2025-06-06T05:23:09.792875Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gameid</th>\n",
       "      <th>participantid</th>\n",
       "      <th>ban1</th>\n",
       "      <th>ban2</th>\n",
       "      <th>...</th>\n",
       "      <th>pick1</th>\n",
       "      <th>patch</th>\n",
       "      <th>teamname</th>\n",
       "      <th>position</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10660-10660_game_1</td>\n",
       "      <td>100</td>\n",
       "      <td>Akali</td>\n",
       "      <td>Nocturne</td>\n",
       "      <td>...</td>\n",
       "      <td>Kalista</td>\n",
       "      <td>13.24</td>\n",
       "      <td>LNG Esports</td>\n",
       "      <td>bot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10660-10660_game_1</td>\n",
       "      <td>200</td>\n",
       "      <td>Poppy</td>\n",
       "      <td>Ashe</td>\n",
       "      <td>...</td>\n",
       "      <td>Renata Glasc</td>\n",
       "      <td>13.24</td>\n",
       "      <td>Rare Atom</td>\n",
       "      <td>sup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10660-10660_game_2</td>\n",
       "      <td>100</td>\n",
       "      <td>Nocturne</td>\n",
       "      <td>Udyr</td>\n",
       "      <td>...</td>\n",
       "      <td>Neeko</td>\n",
       "      <td>13.24</td>\n",
       "      <td>LNG Esports</td>\n",
       "      <td>mid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10660-10660_game_2</td>\n",
       "      <td>200</td>\n",
       "      <td>Poppy</td>\n",
       "      <td>Ashe</td>\n",
       "      <td>...</td>\n",
       "      <td>Kalista</td>\n",
       "      <td>13.24</td>\n",
       "      <td>Rare Atom</td>\n",
       "      <td>bot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10660-10660_game_3</td>\n",
       "      <td>100</td>\n",
       "      <td>Rell</td>\n",
       "      <td>Nocturne</td>\n",
       "      <td>...</td>\n",
       "      <td>Neeko</td>\n",
       "      <td>13.24</td>\n",
       "      <td>LNG Esports</td>\n",
       "      <td>mid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19603</th>\n",
       "      <td>LOLTMNT02_193448</td>\n",
       "      <td>200</td>\n",
       "      <td>Poppy</td>\n",
       "      <td>Skarner</td>\n",
       "      <td>...</td>\n",
       "      <td>Nocturne</td>\n",
       "      <td>14.23</td>\n",
       "      <td>OKSavingsBank BRION</td>\n",
       "      <td>jng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19604</th>\n",
       "      <td>LOLTMNT02_194400</td>\n",
       "      <td>100</td>\n",
       "      <td>LeBlanc</td>\n",
       "      <td>Poppy</td>\n",
       "      <td>...</td>\n",
       "      <td>Skarner</td>\n",
       "      <td>14.23</td>\n",
       "      <td>OKSavingsBank BRION</td>\n",
       "      <td>jng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19605</th>\n",
       "      <td>LOLTMNT02_194400</td>\n",
       "      <td>200</td>\n",
       "      <td>Vi</td>\n",
       "      <td>Nocturne</td>\n",
       "      <td>...</td>\n",
       "      <td>Aurora</td>\n",
       "      <td>14.23</td>\n",
       "      <td>Dplus KIA</td>\n",
       "      <td>mid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19606</th>\n",
       "      <td>LOLTMNT02_194401</td>\n",
       "      <td>100</td>\n",
       "      <td>Vi</td>\n",
       "      <td>Renekton</td>\n",
       "      <td>...</td>\n",
       "      <td>Aurora</td>\n",
       "      <td>14.23</td>\n",
       "      <td>Dplus KIA</td>\n",
       "      <td>mid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19607</th>\n",
       "      <td>LOLTMNT02_194401</td>\n",
       "      <td>200</td>\n",
       "      <td>LeBlanc</td>\n",
       "      <td>Poppy</td>\n",
       "      <td>...</td>\n",
       "      <td>Jax</td>\n",
       "      <td>14.23</td>\n",
       "      <td>OKSavingsBank BRION</td>\n",
       "      <td>top</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19608 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   gameid  participantid      ban1      ban2  ...  \\\n",
       "0      10660-10660_game_1            100     Akali  Nocturne  ...   \n",
       "1      10660-10660_game_1            200     Poppy      Ashe  ...   \n",
       "2      10660-10660_game_2            100  Nocturne      Udyr  ...   \n",
       "3      10660-10660_game_2            200     Poppy      Ashe  ...   \n",
       "4      10660-10660_game_3            100      Rell  Nocturne  ...   \n",
       "...                   ...            ...       ...       ...  ...   \n",
       "19603    LOLTMNT02_193448            200     Poppy   Skarner  ...   \n",
       "19604    LOLTMNT02_194400            100   LeBlanc     Poppy  ...   \n",
       "19605    LOLTMNT02_194400            200        Vi  Nocturne  ...   \n",
       "19606    LOLTMNT02_194401            100        Vi  Renekton  ...   \n",
       "19607    LOLTMNT02_194401            200   LeBlanc     Poppy  ...   \n",
       "\n",
       "              pick1  patch             teamname position  \n",
       "0           Kalista  13.24          LNG Esports      bot  \n",
       "1      Renata Glasc  13.24            Rare Atom      sup  \n",
       "2             Neeko  13.24          LNG Esports      mid  \n",
       "3           Kalista  13.24            Rare Atom      bot  \n",
       "4             Neeko  13.24          LNG Esports      mid  \n",
       "...             ...    ...                  ...      ...  \n",
       "19603      Nocturne  14.23  OKSavingsBank BRION      jng  \n",
       "19604       Skarner  14.23  OKSavingsBank BRION      jng  \n",
       "19605        Aurora  14.23            Dplus KIA      mid  \n",
       "19606        Aurora  14.23            Dplus KIA      mid  \n",
       "19607           Jax  14.23  OKSavingsBank BRION      top  \n",
       "\n",
       "[19608 rows x 11 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We add the position of pick1 to use as training data later. This is done by matching the gameids and champions of a \n",
    "# lookup table with the positions in it with the gameids and pick1s of the team data rows so as to attach the position \n",
    "# corresponding to the champion in the pick1 spot for that game.\n",
    "\n",
    "positions = df[(~df[\"participantid\"].isin([100, 200])) & df[\"position\"].notna()]\n",
    "positions = positions[[\"gameid\", \"champion\", \"position\"]]\n",
    "\n",
    "eval_df = eval_df.merge(positions, left_on=[\"gameid\", \"pick1\"], right_on=[\"gameid\", \"champion\"], how=\"left\")\n",
    "eval_df = eval_df.drop(columns=\"champion\", axis=1)\n",
    "eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T05:23:10.095941Z",
     "start_time": "2025-06-06T05:23:09.947426Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gameid</th>\n",
       "      <th>participantid</th>\n",
       "      <th>ban1</th>\n",
       "      <th>ban2</th>\n",
       "      <th>...</th>\n",
       "      <th>patch</th>\n",
       "      <th>teamname</th>\n",
       "      <th>position</th>\n",
       "      <th>playername</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10660-10660_game_1</td>\n",
       "      <td>100</td>\n",
       "      <td>Akali</td>\n",
       "      <td>Nocturne</td>\n",
       "      <td>...</td>\n",
       "      <td>13.24</td>\n",
       "      <td>LNG Esports</td>\n",
       "      <td>bot</td>\n",
       "      <td>GALA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10660-10660_game_1</td>\n",
       "      <td>200</td>\n",
       "      <td>Poppy</td>\n",
       "      <td>Ashe</td>\n",
       "      <td>...</td>\n",
       "      <td>13.24</td>\n",
       "      <td>Rare Atom</td>\n",
       "      <td>sup</td>\n",
       "      <td>Zorah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10660-10660_game_2</td>\n",
       "      <td>100</td>\n",
       "      <td>Nocturne</td>\n",
       "      <td>Udyr</td>\n",
       "      <td>...</td>\n",
       "      <td>13.24</td>\n",
       "      <td>LNG Esports</td>\n",
       "      <td>mid</td>\n",
       "      <td>Scout</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10660-10660_game_2</td>\n",
       "      <td>200</td>\n",
       "      <td>Poppy</td>\n",
       "      <td>Ashe</td>\n",
       "      <td>...</td>\n",
       "      <td>13.24</td>\n",
       "      <td>Rare Atom</td>\n",
       "      <td>bot</td>\n",
       "      <td>Assum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10660-10660_game_3</td>\n",
       "      <td>100</td>\n",
       "      <td>Rell</td>\n",
       "      <td>Nocturne</td>\n",
       "      <td>...</td>\n",
       "      <td>13.24</td>\n",
       "      <td>LNG Esports</td>\n",
       "      <td>mid</td>\n",
       "      <td>Scout</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19603</th>\n",
       "      <td>LOLTMNT02_193448</td>\n",
       "      <td>200</td>\n",
       "      <td>Poppy</td>\n",
       "      <td>Skarner</td>\n",
       "      <td>...</td>\n",
       "      <td>14.23</td>\n",
       "      <td>OKSavingsBank BRION</td>\n",
       "      <td>jng</td>\n",
       "      <td>HamBak</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19604</th>\n",
       "      <td>LOLTMNT02_194400</td>\n",
       "      <td>100</td>\n",
       "      <td>LeBlanc</td>\n",
       "      <td>Poppy</td>\n",
       "      <td>...</td>\n",
       "      <td>14.23</td>\n",
       "      <td>OKSavingsBank BRION</td>\n",
       "      <td>jng</td>\n",
       "      <td>HamBak</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19605</th>\n",
       "      <td>LOLTMNT02_194400</td>\n",
       "      <td>200</td>\n",
       "      <td>Vi</td>\n",
       "      <td>Nocturne</td>\n",
       "      <td>...</td>\n",
       "      <td>14.23</td>\n",
       "      <td>Dplus KIA</td>\n",
       "      <td>mid</td>\n",
       "      <td>ShowMaker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19606</th>\n",
       "      <td>LOLTMNT02_194401</td>\n",
       "      <td>100</td>\n",
       "      <td>Vi</td>\n",
       "      <td>Renekton</td>\n",
       "      <td>...</td>\n",
       "      <td>14.23</td>\n",
       "      <td>Dplus KIA</td>\n",
       "      <td>mid</td>\n",
       "      <td>ShowMaker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19607</th>\n",
       "      <td>LOLTMNT02_194401</td>\n",
       "      <td>200</td>\n",
       "      <td>LeBlanc</td>\n",
       "      <td>Poppy</td>\n",
       "      <td>...</td>\n",
       "      <td>14.23</td>\n",
       "      <td>OKSavingsBank BRION</td>\n",
       "      <td>top</td>\n",
       "      <td>Morgan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19608 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   gameid  participantid      ban1      ban2  ...  patch  \\\n",
       "0      10660-10660_game_1            100     Akali  Nocturne  ...  13.24   \n",
       "1      10660-10660_game_1            200     Poppy      Ashe  ...  13.24   \n",
       "2      10660-10660_game_2            100  Nocturne      Udyr  ...  13.24   \n",
       "3      10660-10660_game_2            200     Poppy      Ashe  ...  13.24   \n",
       "4      10660-10660_game_3            100      Rell  Nocturne  ...  13.24   \n",
       "...                   ...            ...       ...       ...  ...    ...   \n",
       "19603    LOLTMNT02_193448            200     Poppy   Skarner  ...  14.23   \n",
       "19604    LOLTMNT02_194400            100   LeBlanc     Poppy  ...  14.23   \n",
       "19605    LOLTMNT02_194400            200        Vi  Nocturne  ...  14.23   \n",
       "19606    LOLTMNT02_194401            100        Vi  Renekton  ...  14.23   \n",
       "19607    LOLTMNT02_194401            200   LeBlanc     Poppy  ...  14.23   \n",
       "\n",
       "                  teamname position playername  \n",
       "0              LNG Esports      bot       GALA  \n",
       "1                Rare Atom      sup      Zorah  \n",
       "2              LNG Esports      mid      Scout  \n",
       "3                Rare Atom      bot      Assum  \n",
       "4              LNG Esports      mid      Scout  \n",
       "...                    ...      ...        ...  \n",
       "19603  OKSavingsBank BRION      jng     HamBak  \n",
       "19604  OKSavingsBank BRION      jng     HamBak  \n",
       "19605            Dplus KIA      mid  ShowMaker  \n",
       "19606            Dplus KIA      mid  ShowMaker  \n",
       "19607  OKSavingsBank BRION      top     Morgan  \n",
       "\n",
       "[19608 rows x 12 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We add the player name to use as training data later. This is achieved the same way as we added pick1.\n",
    "\n",
    "players = df[(~df[\"participantid\"].isin([100, 200])) & df[\"playername\"].notna()]\n",
    "players = players[[\"gameid\", \"champion\", \"playername\"]]\n",
    "\n",
    "eval_df = eval_df.merge(players, left_on=[\"gameid\", \"pick1\"], right_on=[\"gameid\", \"champion\"], how=\"left\")\n",
    "eval_df = eval_df.drop(columns=\"champion\", axis=1)\n",
    "eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T05:23:10.117097Z",
     "start_time": "2025-06-06T05:23:10.097950Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gameid</th>\n",
       "      <th>ban6</th>\n",
       "      <th>ban7</th>\n",
       "      <th>ban8</th>\n",
       "      <th>ban9</th>\n",
       "      <th>ban10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10660-10660_game_1</td>\n",
       "      <td>Poppy</td>\n",
       "      <td>Ashe</td>\n",
       "      <td>Neeko</td>\n",
       "      <td>Vi</td>\n",
       "      <td>Jarvan IV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10660-10660_game_2</td>\n",
       "      <td>Poppy</td>\n",
       "      <td>Ashe</td>\n",
       "      <td>Rumble</td>\n",
       "      <td>Tristana</td>\n",
       "      <td>Lucian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10660-10660_game_3</td>\n",
       "      <td>Poppy</td>\n",
       "      <td>Ashe</td>\n",
       "      <td>LeBlanc</td>\n",
       "      <td>Sejuani</td>\n",
       "      <td>Vi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10660-10660_game_4</td>\n",
       "      <td>Rell</td>\n",
       "      <td>Nocturne</td>\n",
       "      <td>Ashe</td>\n",
       "      <td>Azir</td>\n",
       "      <td>Akali</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10661-10661_game_1</td>\n",
       "      <td>Kalista</td>\n",
       "      <td>Nocturne</td>\n",
       "      <td>Neeko</td>\n",
       "      <td>Sejuani</td>\n",
       "      <td>Poppy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19599</th>\n",
       "      <td>LOLTMNT02_194058</td>\n",
       "      <td>K'Sante</td>\n",
       "      <td>Aurora</td>\n",
       "      <td>Skarner</td>\n",
       "      <td>Renekton</td>\n",
       "      <td>Maokai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19601</th>\n",
       "      <td>LOLTMNT02_193442</td>\n",
       "      <td>Aurora</td>\n",
       "      <td>Skarner</td>\n",
       "      <td>Ambessa</td>\n",
       "      <td>Renekton</td>\n",
       "      <td>Rakan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19603</th>\n",
       "      <td>LOLTMNT02_193448</td>\n",
       "      <td>Poppy</td>\n",
       "      <td>Skarner</td>\n",
       "      <td>Ambessa</td>\n",
       "      <td>Ashe</td>\n",
       "      <td>Varus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19605</th>\n",
       "      <td>LOLTMNT02_194400</td>\n",
       "      <td>Vi</td>\n",
       "      <td>Nocturne</td>\n",
       "      <td>Azir</td>\n",
       "      <td>Gragas</td>\n",
       "      <td>Maokai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19607</th>\n",
       "      <td>LOLTMNT02_194401</td>\n",
       "      <td>LeBlanc</td>\n",
       "      <td>Poppy</td>\n",
       "      <td>Ambessa</td>\n",
       "      <td>Nocturne</td>\n",
       "      <td>Lee Sin</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9804 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   gameid     ban6      ban7     ban8      ban9      ban10\n",
       "1      10660-10660_game_1    Poppy      Ashe    Neeko        Vi  Jarvan IV\n",
       "3      10660-10660_game_2    Poppy      Ashe   Rumble  Tristana     Lucian\n",
       "5      10660-10660_game_3    Poppy      Ashe  LeBlanc   Sejuani         Vi\n",
       "7      10660-10660_game_4     Rell  Nocturne     Ashe      Azir      Akali\n",
       "9      10661-10661_game_1  Kalista  Nocturne    Neeko   Sejuani      Poppy\n",
       "...                   ...      ...       ...      ...       ...        ...\n",
       "19599    LOLTMNT02_194058  K'Sante    Aurora  Skarner  Renekton     Maokai\n",
       "19601    LOLTMNT02_193442   Aurora   Skarner  Ambessa  Renekton      Rakan\n",
       "19603    LOLTMNT02_193448    Poppy   Skarner  Ambessa      Ashe      Varus\n",
       "19605    LOLTMNT02_194400       Vi  Nocturne     Azir    Gragas     Maokai\n",
       "19607    LOLTMNT02_194401  LeBlanc     Poppy  Ambessa  Nocturne    Lee Sin\n",
       "\n",
       "[9804 rows x 6 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We prepare the other team's bans in the same game to be added to our information. \n",
    "# We do this by reversing participantid in a copy dataframe and then extracting all bans with participantid labeled 100 (in reality, 200), \n",
    "# and then renaming their ban columns from ban1-5 to ban 6-10.\n",
    "\n",
    "opponent_bans = eval_df[[\"gameid\", \"participantid\", \"ban1\", \"ban2\", \"ban3\", \"ban4\", \"ban5\"]].copy()\n",
    "opponent_bans[\"participantid\"] = opponent_bans[\"participantid\"].map({100: 200, 200: 100})\n",
    "opponent_bans = opponent_bans[opponent_bans[\"participantid\"] == 100]\n",
    "opponent_bans = opponent_bans.drop(columns=[\"participantid\"])\n",
    "\n",
    "opponent_bans = opponent_bans.rename(columns={\n",
    "    \"ban1\": \"ban6\",\n",
    "    \"ban2\": \"ban7\",\n",
    "    \"ban3\": \"ban8\",\n",
    "    \"ban4\": \"ban9\",\n",
    "    \"ban5\": \"ban10\",\n",
    "})\n",
    "\n",
    "opponent_bans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T05:23:10.137978Z",
     "start_time": "2025-06-06T05:23:10.119608Z"
    }
   },
   "outputs": [],
   "source": [
    "# We merge the other team's bans in the same game into our DataFrame, on participantid which we manipulated in the previous cell, \n",
    "# and eliminate the duplicate rows.\n",
    "\n",
    "eval_df = eval_df.merge(opponent_bans, on=\"gameid\", how=\"left\")\n",
    "eval_df = eval_df[eval_df[\"participantid\"] == 100]\n",
    "eval_df = eval_df.drop(columns=[\"gameid\", \"participantid\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T05:23:10.165333Z",
     "start_time": "2025-06-06T05:23:10.139985Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ban1</th>\n",
       "      <th>ban2</th>\n",
       "      <th>ban3</th>\n",
       "      <th>ban4</th>\n",
       "      <th>...</th>\n",
       "      <th>ban7</th>\n",
       "      <th>ban8</th>\n",
       "      <th>ban9</th>\n",
       "      <th>ban10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Akali</td>\n",
       "      <td>Nocturne</td>\n",
       "      <td>K'Sante</td>\n",
       "      <td>Lee Sin</td>\n",
       "      <td>...</td>\n",
       "      <td>Ashe</td>\n",
       "      <td>Neeko</td>\n",
       "      <td>Vi</td>\n",
       "      <td>Jarvan IV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nocturne</td>\n",
       "      <td>Udyr</td>\n",
       "      <td>Renata Glasc</td>\n",
       "      <td>Nautilus</td>\n",
       "      <td>...</td>\n",
       "      <td>Ashe</td>\n",
       "      <td>Rumble</td>\n",
       "      <td>Tristana</td>\n",
       "      <td>Lucian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Rell</td>\n",
       "      <td>Nocturne</td>\n",
       "      <td>Tristana</td>\n",
       "      <td>Jarvan IV</td>\n",
       "      <td>...</td>\n",
       "      <td>Ashe</td>\n",
       "      <td>LeBlanc</td>\n",
       "      <td>Sejuani</td>\n",
       "      <td>Vi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Poppy</td>\n",
       "      <td>LeBlanc</td>\n",
       "      <td>Neeko</td>\n",
       "      <td>Sejuani</td>\n",
       "      <td>...</td>\n",
       "      <td>Nocturne</td>\n",
       "      <td>Ashe</td>\n",
       "      <td>Azir</td>\n",
       "      <td>Akali</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ashe</td>\n",
       "      <td>Akali</td>\n",
       "      <td>LeBlanc</td>\n",
       "      <td>Vi</td>\n",
       "      <td>...</td>\n",
       "      <td>Nocturne</td>\n",
       "      <td>Neeko</td>\n",
       "      <td>Sejuani</td>\n",
       "      <td>Poppy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8620</th>\n",
       "      <td>Sejuani</td>\n",
       "      <td>Vi</td>\n",
       "      <td>Ambessa</td>\n",
       "      <td>LeBlanc</td>\n",
       "      <td>...</td>\n",
       "      <td>Aurora</td>\n",
       "      <td>Skarner</td>\n",
       "      <td>Renekton</td>\n",
       "      <td>Maokai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8621</th>\n",
       "      <td>LeBlanc</td>\n",
       "      <td>Poppy</td>\n",
       "      <td>Azir</td>\n",
       "      <td>Bard</td>\n",
       "      <td>...</td>\n",
       "      <td>Skarner</td>\n",
       "      <td>Ambessa</td>\n",
       "      <td>Renekton</td>\n",
       "      <td>Rakan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8622</th>\n",
       "      <td>Renekton</td>\n",
       "      <td>Vi</td>\n",
       "      <td>K'Sante</td>\n",
       "      <td>Kalista</td>\n",
       "      <td>...</td>\n",
       "      <td>Skarner</td>\n",
       "      <td>Ambessa</td>\n",
       "      <td>Ashe</td>\n",
       "      <td>Varus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8623</th>\n",
       "      <td>LeBlanc</td>\n",
       "      <td>Poppy</td>\n",
       "      <td>Ambessa</td>\n",
       "      <td>Lee Sin</td>\n",
       "      <td>...</td>\n",
       "      <td>Nocturne</td>\n",
       "      <td>Azir</td>\n",
       "      <td>Gragas</td>\n",
       "      <td>Maokai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8624</th>\n",
       "      <td>Vi</td>\n",
       "      <td>Renekton</td>\n",
       "      <td>Skarner</td>\n",
       "      <td>Ashe</td>\n",
       "      <td>...</td>\n",
       "      <td>Poppy</td>\n",
       "      <td>Ambessa</td>\n",
       "      <td>Nocturne</td>\n",
       "      <td>Lee Sin</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8625 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ban1      ban2          ban3       ban4  ...      ban7     ban8  \\\n",
       "0        Akali  Nocturne       K'Sante    Lee Sin  ...      Ashe    Neeko   \n",
       "1     Nocturne      Udyr  Renata Glasc   Nautilus  ...      Ashe   Rumble   \n",
       "2         Rell  Nocturne      Tristana  Jarvan IV  ...      Ashe  LeBlanc   \n",
       "3        Poppy   LeBlanc         Neeko    Sejuani  ...  Nocturne     Ashe   \n",
       "4         Ashe     Akali       LeBlanc         Vi  ...  Nocturne    Neeko   \n",
       "...        ...       ...           ...        ...  ...       ...      ...   \n",
       "8620   Sejuani        Vi       Ambessa    LeBlanc  ...    Aurora  Skarner   \n",
       "8621   LeBlanc     Poppy          Azir       Bard  ...   Skarner  Ambessa   \n",
       "8622  Renekton        Vi       K'Sante    Kalista  ...   Skarner  Ambessa   \n",
       "8623   LeBlanc     Poppy       Ambessa    Lee Sin  ...  Nocturne     Azir   \n",
       "8624        Vi  Renekton       Skarner       Ashe  ...     Poppy  Ambessa   \n",
       "\n",
       "          ban9      ban10  \n",
       "0           Vi  Jarvan IV  \n",
       "1     Tristana     Lucian  \n",
       "2      Sejuani         Vi  \n",
       "3         Azir      Akali  \n",
       "4      Sejuani      Poppy  \n",
       "...        ...        ...  \n",
       "8620  Renekton     Maokai  \n",
       "8621  Renekton      Rakan  \n",
       "8622      Ashe      Varus  \n",
       "8623    Gragas     Maokai  \n",
       "8624  Nocturne    Lee Sin  \n",
       "\n",
       "[8625 rows x 15 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We drop NaN rows. Our DataFrame is now ready.\n",
    "\n",
    "eval_df = eval_df.dropna().reset_index(drop=True)\n",
    "eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T05:23:12.443608Z",
     "start_time": "2025-06-06T05:23:10.167340Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gameid</th>\n",
       "      <th>datacompleteness</th>\n",
       "      <th>url</th>\n",
       "      <th>league</th>\n",
       "      <th>...</th>\n",
       "      <th>deathsat25</th>\n",
       "      <th>opp_killsat25</th>\n",
       "      <th>opp_assistsat25</th>\n",
       "      <th>opp_deathsat25</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10660-10660_game_1</td>\n",
       "      <td>partial</td>\n",
       "      <td>https://lpl.qq.com/es/stats.shtml?bmid=10660</td>\n",
       "      <td>DCup</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10660-10660_game_1</td>\n",
       "      <td>partial</td>\n",
       "      <td>https://lpl.qq.com/es/stats.shtml?bmid=10660</td>\n",
       "      <td>DCup</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10660-10660_game_1</td>\n",
       "      <td>partial</td>\n",
       "      <td>https://lpl.qq.com/es/stats.shtml?bmid=10660</td>\n",
       "      <td>DCup</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10660-10660_game_1</td>\n",
       "      <td>partial</td>\n",
       "      <td>https://lpl.qq.com/es/stats.shtml?bmid=10660</td>\n",
       "      <td>DCup</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10660-10660_game_1</td>\n",
       "      <td>partial</td>\n",
       "      <td>https://lpl.qq.com/es/stats.shtml?bmid=10660</td>\n",
       "      <td>DCup</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10660-10660_game_1</td>\n",
       "      <td>partial</td>\n",
       "      <td>https://lpl.qq.com/es/stats.shtml?bmid=10660</td>\n",
       "      <td>DCup</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10660-10660_game_1</td>\n",
       "      <td>partial</td>\n",
       "      <td>https://lpl.qq.com/es/stats.shtml?bmid=10660</td>\n",
       "      <td>DCup</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10660-10660_game_1</td>\n",
       "      <td>partial</td>\n",
       "      <td>https://lpl.qq.com/es/stats.shtml?bmid=10660</td>\n",
       "      <td>DCup</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10660-10660_game_1</td>\n",
       "      <td>partial</td>\n",
       "      <td>https://lpl.qq.com/es/stats.shtml?bmid=10660</td>\n",
       "      <td>DCup</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10660-10660_game_1</td>\n",
       "      <td>partial</td>\n",
       "      <td>https://lpl.qq.com/es/stats.shtml?bmid=10660</td>\n",
       "      <td>DCup</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10660-10660_game_1</td>\n",
       "      <td>partial</td>\n",
       "      <td>https://lpl.qq.com/es/stats.shtml?bmid=10660</td>\n",
       "      <td>DCup</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10660-10660_game_1</td>\n",
       "      <td>partial</td>\n",
       "      <td>https://lpl.qq.com/es/stats.shtml?bmid=10660</td>\n",
       "      <td>DCup</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12 rows × 161 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                gameid datacompleteness  \\\n",
       "0   10660-10660_game_1          partial   \n",
       "1   10660-10660_game_1          partial   \n",
       "2   10660-10660_game_1          partial   \n",
       "3   10660-10660_game_1          partial   \n",
       "4   10660-10660_game_1          partial   \n",
       "5   10660-10660_game_1          partial   \n",
       "6   10660-10660_game_1          partial   \n",
       "7   10660-10660_game_1          partial   \n",
       "8   10660-10660_game_1          partial   \n",
       "9   10660-10660_game_1          partial   \n",
       "10  10660-10660_game_1          partial   \n",
       "11  10660-10660_game_1          partial   \n",
       "\n",
       "                                             url league  ...  deathsat25  \\\n",
       "0   https://lpl.qq.com/es/stats.shtml?bmid=10660   DCup  ...         NaN   \n",
       "1   https://lpl.qq.com/es/stats.shtml?bmid=10660   DCup  ...         NaN   \n",
       "2   https://lpl.qq.com/es/stats.shtml?bmid=10660   DCup  ...         NaN   \n",
       "3   https://lpl.qq.com/es/stats.shtml?bmid=10660   DCup  ...         NaN   \n",
       "4   https://lpl.qq.com/es/stats.shtml?bmid=10660   DCup  ...         NaN   \n",
       "5   https://lpl.qq.com/es/stats.shtml?bmid=10660   DCup  ...         NaN   \n",
       "6   https://lpl.qq.com/es/stats.shtml?bmid=10660   DCup  ...         NaN   \n",
       "7   https://lpl.qq.com/es/stats.shtml?bmid=10660   DCup  ...         NaN   \n",
       "8   https://lpl.qq.com/es/stats.shtml?bmid=10660   DCup  ...         NaN   \n",
       "9   https://lpl.qq.com/es/stats.shtml?bmid=10660   DCup  ...         NaN   \n",
       "10  https://lpl.qq.com/es/stats.shtml?bmid=10660   DCup  ...         NaN   \n",
       "11  https://lpl.qq.com/es/stats.shtml?bmid=10660   DCup  ...         NaN   \n",
       "\n",
       "   opp_killsat25  opp_assistsat25 opp_deathsat25  \n",
       "0            NaN              NaN            NaN  \n",
       "1            NaN              NaN            NaN  \n",
       "2            NaN              NaN            NaN  \n",
       "3            NaN              NaN            NaN  \n",
       "4            NaN              NaN            NaN  \n",
       "5            NaN              NaN            NaN  \n",
       "6            NaN              NaN            NaN  \n",
       "7            NaN              NaN            NaN  \n",
       "8            NaN              NaN            NaN  \n",
       "9            NaN              NaN            NaN  \n",
       "10           NaN              NaN            NaN  \n",
       "11           NaN              NaN            NaN  \n",
       "\n",
       "[12 rows x 161 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We re-read the default csv for all our other later analysis.\n",
    "\n",
    "df = pd.read_csv(\"2024_LoL_esports_match_data_from_OraclesElixir.csv\", low_memory=False)\n",
    "df.head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T05:23:12.939246Z",
     "start_time": "2025-06-06T05:23:12.445152Z"
    }
   },
   "outputs": [],
   "source": [
    "# Univariate Analysis: Top 30 Picked Champions\n",
    "# This selects and then graphs the Top 30 Picked Champions. I wanted to see if there were any outliers, \n",
    "# and I'm curious if my predictor later will also numerically favour the Champions whom are the most common in these data as a whole. \n",
    "# I think that graphing is generally pretty self explanatory.\n",
    "\n",
    "champion_counts = df[\"champion\"].value_counts().reset_index()\n",
    "champion_counts.columns = [\"champion\", \"count\"]\n",
    "\n",
    "top_champions = champion_counts.head(30)\n",
    "\n",
    "fig = px.bar(\n",
    "    top_champions,\n",
    "    x=\"champion\",\n",
    "    y=\"count\",\n",
    "    title=\"Top 30 Champions By Pick Count\"\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    width=1000,\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T05:23:13.191737Z",
     "start_time": "2025-06-06T05:23:12.942255Z"
    }
   },
   "outputs": [],
   "source": [
    "# Univariate Analysis: Top 30 Banned Champions\n",
    "# This selects and then graphs the Top 30 Banned Champions. I wanted to see if there were any outliers, and I'm curious if my predictor later will also be the most influenced by the Champions whom are the most common in these data as a whole.\n",
    "# We use eval_df, which only has one row per game, to make sure we don't repeat the bans for each player entry. I think that graphing is generally pretty self explanatory.\n",
    "\n",
    "all_bans = pd.concat([eval_df[col] for col in ['ban1', 'ban2', 'ban3', 'ban4', 'ban5']], axis=0)\n",
    "ban_counts = all_bans.dropna().value_counts().reset_index()\n",
    "ban_counts.columns = [\"champion\", \"count\"]\n",
    "\n",
    "top_bans = ban_counts.head(30)\n",
    "\n",
    "fig = px.bar(\n",
    "    top_bans,\n",
    "    x=\"champion\",\n",
    "    y=\"count\",\n",
    "    title=\"Top 30 Champions By Ban Count\"\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    width=1000, \n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T05:23:13.630572Z",
     "start_time": "2025-06-06T05:23:13.193747Z"
    }
   },
   "outputs": [],
   "source": [
    "# Bivariate Analysis: Arcane-Themed Pick vs. Ban Heatmap\n",
    "# This selects and then graphs a heatmap for the Champions who appeared in Arcane. Every entry in this heatmap shows, when the Champion in that entry was picked, how many times they banned the Champion on the other side of the table. I wanted to see if the bans were roughly evenly distributed, or if some Champions are just never banned with certain picks. I think that graphing is generally self explanatory.\n",
    "\n",
    "champion_subset = [\"Caitlyn\", \"Ekko\", \"Heimerdinger\", \"Jayce\", \"Jinx\", \"Leblanc\", \"Orianna\", \"Singed\", \"Vi\", \"Viktor\", \"Warwick\"]\n",
    "\n",
    "# Concatenate all picks and bans.\n",
    "pick_ban_df = pd.DataFrame({\n",
    "    \"pick\": pd.concat([df[col] for col in [\"pick1\", \"pick2\", \"pick3\", \"pick4\", \"pick5\"]], ignore_index=True),\n",
    "    \"ban\": pd.concat([df[col] for col in [\"ban1\", \"ban2\", \"ban3\", \"ban4\", \"ban5\"]], ignore_index=True)\n",
    "})\n",
    "\n",
    "# Only keep the ones for the champions we want to investigate.\n",
    "pick_ban_df = pick_ban_df[pick_ban_df[\"pick\"].isin(champion_subset) & pick_ban_df[\"ban\"].isin(champion_subset)]\n",
    "\n",
    "freqs = pd.crosstab(pick_ban_df[\"pick\"], pick_ban_df[\"ban\"])\n",
    "fig = px.imshow(\n",
    "    freqs,\n",
    "    labels=dict(x=\"Banned Champion\", y=\"Picked Champion\", color=\"Frequency\"),\n",
    "    title=\"Pick vs Ban Frequency for Arcane's Champions\",\n",
    "    aspect=\"auto\"\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    width=900, \n",
    "    height=800\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T05:23:13.935800Z",
     "start_time": "2025-06-06T05:23:13.632577Z"
    }
   },
   "outputs": [],
   "source": [
    "# Bivariate Analysis: Ship-Themed Pick vs. Ban Percentile Heatmap\n",
    "# This selects and then graphs a percentile heatmap for some Champions who are shipped together. Every entry in this heatmap shows, when the Champion in that entry was picked, how many times they banned the Champion on the other side of the table. I wanted to see if the bans were roughly evenly distributed, or if some Champions are just never banned with certain picks. I think that graphing is generally self explanatory. See previous cell for almost identical explanation.\n",
    "\n",
    "champion_subset = [\"Graves\", \"Twisted Fate\", \"Yasuo\", \"Ahri\", \"Braum\", \"Illaoi\", \"Diana\", \"Leona\", \"Jinx\", \"Ekko\", \"Caitlyn\", \"Violet\", \"Garen\", \"Lux\"]\n",
    "\n",
    "# Concatenate all picks and bans.\n",
    "pick_ban_df = pd.DataFrame({\n",
    "    \"pick\": pd.concat([df[col] for col in [\"pick1\", \"pick2\", \"pick3\", \"pick4\", \"pick5\"]], ignore_index=True),\n",
    "    \"ban\": pd.concat([df[col] for col in [\"ban1\", \"ban2\", \"ban3\", \"ban4\", \"ban5\"]], ignore_index=True)\n",
    "})\n",
    "\n",
    "# Only keep the ones for the champions we want to investigate.\n",
    "pick_ban_df = pick_ban_df[pick_ban_df[\"pick\"].isin(champion_subset) & pick_ban_df[\"ban\"].isin(champion_subset)]\n",
    "\n",
    "# normalize = True is what makes this different from the previous one\n",
    "freqs = pd.crosstab(pick_ban_df[\"pick\"], pick_ban_df[\"ban\"], normalize=True)\n",
    "fig = px.imshow(\n",
    "    freqs,\n",
    "    labels=dict(x=\"Banned Champion\", y=\"Picked Champion\", color=\"Frequency\"),\n",
    "    title=\"Pick vs Ban Frequency for Shipped Champions\",\n",
    "    aspect=\"auto\"\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    width=900, \n",
    "    height=800\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T05:23:14.089470Z",
     "start_time": "2025-06-06T05:23:13.937805Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Champion bans by side:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>side</th>\n",
       "      <th>Blue</th>\n",
       "      <th>Red</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>champion</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Aatrox</th>\n",
       "      <td>1842</td>\n",
       "      <td>1644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ahri</th>\n",
       "      <td>1812</td>\n",
       "      <td>1794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Akali</th>\n",
       "      <td>1950</td>\n",
       "      <td>1776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Akshan</th>\n",
       "      <td>102</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Alistar</th>\n",
       "      <td>2064</td>\n",
       "      <td>2742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Zeri</th>\n",
       "      <td>2400</td>\n",
       "      <td>1710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ziggs</th>\n",
       "      <td>2748</td>\n",
       "      <td>2778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Zilean</th>\n",
       "      <td>18</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Zoe</th>\n",
       "      <td>138</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Zyra</th>\n",
       "      <td>2436</td>\n",
       "      <td>2898</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>167 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "side      Blue   Red\n",
       "champion            \n",
       "Aatrox    1842  1644\n",
       "Ahri      1812  1794\n",
       "Akali     1950  1776\n",
       "Akshan     102    78\n",
       "Alistar   2064  2742\n",
       "...        ...   ...\n",
       "Zeri      2400  1710\n",
       "Ziggs     2748  2778\n",
       "Zilean      18    12\n",
       "Zoe        138   108\n",
       "Zyra      2436  2898\n",
       "\n",
       "[167 rows x 2 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pivot Table: Banned Champions By Side\n",
    "# This concatenates the champion ban list while distinguishing which side banned them, then renders it as a pivot table. I wanted to see if there were significant differences here that should be taken into account.\n",
    "\n",
    "ban_side_df = pd.concat([df[[col, \"side\"]].rename(columns={col: \"champion\"}) \n",
    "                         for col in [\"ban1\", \"ban2\", \"ban3\", \"ban4\", \"ban5\"]], ignore_index=True, axis=0)\n",
    "\n",
    "pivot = pd.pivot_table(ban_side_df, index=\"champion\", columns=\"side\", aggfunc=\"size\", fill_value=0)\n",
    "print(\"Champion bans by side:\")\n",
    "pivot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Assessment of Missingness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T05:23:16.476785Z",
     "start_time": "2025-06-06T05:23:14.091483Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gameid</th>\n",
       "      <th>datacompleteness</th>\n",
       "      <th>url</th>\n",
       "      <th>league</th>\n",
       "      <th>...</th>\n",
       "      <th>deathsat25</th>\n",
       "      <th>opp_killsat25</th>\n",
       "      <th>opp_assistsat25</th>\n",
       "      <th>opp_deathsat25</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>LOLTMNT99_132542</td>\n",
       "      <td>complete</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TSC</td>\n",
       "      <td>...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>LOLTMNT99_132542</td>\n",
       "      <td>complete</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TSC</td>\n",
       "      <td>...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>LOLTMNT99_132665</td>\n",
       "      <td>complete</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TSC</td>\n",
       "      <td>...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>LOLTMNT99_132665</td>\n",
       "      <td>complete</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TSC</td>\n",
       "      <td>...</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>LOLTMNT99_132755</td>\n",
       "      <td>complete</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TSC</td>\n",
       "      <td>...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117623</th>\n",
       "      <td>LOLTMNT02_193448</td>\n",
       "      <td>complete</td>\n",
       "      <td>NaN</td>\n",
       "      <td>KeSPA</td>\n",
       "      <td>...</td>\n",
       "      <td>19.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117634</th>\n",
       "      <td>LOLTMNT02_194400</td>\n",
       "      <td>complete</td>\n",
       "      <td>NaN</td>\n",
       "      <td>KeSPA</td>\n",
       "      <td>...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117635</th>\n",
       "      <td>LOLTMNT02_194400</td>\n",
       "      <td>complete</td>\n",
       "      <td>NaN</td>\n",
       "      <td>KeSPA</td>\n",
       "      <td>...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117646</th>\n",
       "      <td>LOLTMNT02_194401</td>\n",
       "      <td>complete</td>\n",
       "      <td>NaN</td>\n",
       "      <td>KeSPA</td>\n",
       "      <td>...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117647</th>\n",
       "      <td>LOLTMNT02_194401</td>\n",
       "      <td>complete</td>\n",
       "      <td>NaN</td>\n",
       "      <td>KeSPA</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16826 rows × 161 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  gameid datacompleteness  url league  ...  deathsat25  \\\n",
       "190     LOLTMNT99_132542         complete  NaN    TSC  ...         7.0   \n",
       "191     LOLTMNT99_132542         complete  NaN    TSC  ...        20.0   \n",
       "202     LOLTMNT99_132665         complete  NaN    TSC  ...        11.0   \n",
       "203     LOLTMNT99_132665         complete  NaN    TSC  ...        17.0   \n",
       "214     LOLTMNT99_132755         complete  NaN    TSC  ...         7.0   \n",
       "...                  ...              ...  ...    ...  ...         ...   \n",
       "117623  LOLTMNT02_193448         complete  NaN  KeSPA  ...        19.0   \n",
       "117634  LOLTMNT02_194400         complete  NaN  KeSPA  ...        11.0   \n",
       "117635  LOLTMNT02_194400         complete  NaN  KeSPA  ...        11.0   \n",
       "117646  LOLTMNT02_194401         complete  NaN  KeSPA  ...         7.0   \n",
       "117647  LOLTMNT02_194401         complete  NaN  KeSPA  ...         8.0   \n",
       "\n",
       "       opp_killsat25  opp_assistsat25 opp_deathsat25  \n",
       "190              7.0             14.0           20.0  \n",
       "191             20.0             47.0            7.0  \n",
       "202             11.0             15.0           17.0  \n",
       "203             17.0             28.0           11.0  \n",
       "214              7.0             11.0           10.0  \n",
       "...              ...              ...            ...  \n",
       "117623          19.0             49.0            6.0  \n",
       "117634          11.0             26.0           11.0  \n",
       "117635          11.0             37.0           11.0  \n",
       "117646           7.0              9.0            8.0  \n",
       "117647           8.0             16.0            7.0  \n",
       "\n",
       "[16826 rows x 161 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We reload the default CSV just to be safe for our assessment of missingness.\n",
    "\n",
    "df = pd.read_csv(\"2024_LoL_esports_match_data_from_OraclesElixir.csv\", low_memory=False)\n",
    "df[df[\"elders\"].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T05:24:14.575532Z",
     "start_time": "2025-06-06T05:23:16.478794Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p: 0.0000000000\n",
      "The value of the column 'elders' is dependent on the value of the column 'participantid'.\n"
     ]
    }
   ],
   "source": [
    "# Missingness Dependency: \"elders\" depends on \"participantid\"\n",
    "# This encodes whether elders are missing or not as the values 0 or 1, and then encodes each position with a different integer value. It then performs permutation tests and assesses the Kolmogorov-Smirnov statistic using scipy to determine the p-value of the distribution. This technique is taken from lecture.\n",
    "# I know the missingness of \"elders\" depends on \"participantid\", because \"elders\" is only recorded for whole-game rows, meaning only rows where \"participantid\" is equal to 100 or 200. It should be strictly completely dependent.\n",
    "\n",
    "col1 = \"elders\"\n",
    "col2 = \"participantid\"\n",
    "\n",
    "n_repetitions = 1000\n",
    "df[f\"missing_{col1}\"] = df[col1].isna().astype(int)\n",
    "\n",
    "x = df[f\"{col2}\"].dropna()\n",
    "group = df.loc[x.index, f\"missing_{col1}\"]\n",
    "\n",
    "observed_ks = ks_2samp(x[group == 1], x[group == 0]).statistic\n",
    "\n",
    "ks_stats = []\n",
    "for _ in range(n_repetitions):\n",
    "    shuffled = np.random.permutation(group)\n",
    "    ks_stat = ks_2samp(x[shuffled == 1], x[shuffled == 0]).statistic\n",
    "    ks_stats.append(ks_stat)\n",
    "\n",
    "p = np.mean(np.array(ks_stats) >= observed_ks)\n",
    "\n",
    "print(f\"p: {p:.10f}\")\n",
    "\n",
    "if p < 0.05:\n",
    "    print(f\"The value of the column '{col1}' is dependent on the value of the column '{col2}'.\")\n",
    "else:\n",
    "    print(f\"The value of the column '{col1}' is not dependent on the value of the column '{col2}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T05:24:55.560352Z",
     "start_time": "2025-06-06T05:24:14.578633Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p: 0.9970000000\n",
      "The value of the column 'elders' is not dependent on the value of the column 'patch'.\n"
     ]
    }
   ],
   "source": [
    "# Missingness Dependency: \"elders\" does not depend on \"patch\"\n",
    "# Same code as test in cell above, same idea. This technique is taken from lecture.\n",
    "# I know the missingness of \"elders\" does not depend on \"patch\", because whether or not \"elders\" is recorded does not change with game updates. The boss recorded in \"elders\", Elder Dragons, exists across all patches of this year's dataset.\n",
    "\n",
    "col1 = \"elders\"\n",
    "col2 = \"patch\"\n",
    "\n",
    "n_repetitions = 1000\n",
    "df[f\"missing_{col1}\"] = df[col1].isna().astype(int)\n",
    "\n",
    "x = df[\"result\"].dropna()\n",
    "group = df.loc[x.index, f\"missing_{col1}\"]\n",
    "\n",
    "observed_ks = ks_2samp(x[group == 1], x[group == 0]).statistic\n",
    "\n",
    "ks_stats = []\n",
    "for _ in range(n_repetitions):\n",
    "    shuffled = np.random.permutation(group)\n",
    "    ks_stat = ks_2samp(x[shuffled == 1], x[shuffled == 0]).statistic\n",
    "    ks_stats.append(ks_stat)\n",
    "\n",
    "p = np.mean(np.array(ks_stats) >= observed_ks)\n",
    "\n",
    "print(f\"p: {p:.10f}\")\n",
    "\n",
    "if p < 0.05:\n",
    "    print(f\"The value of the column '{col1}' is dependent on the value of the column '{col2}'.\")\n",
    "else:\n",
    "    print(f\"The value of the column '{col1}' is not dependent on the value of the column '{col2}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T05:24:55.862035Z",
     "start_time": "2025-06-06T05:24:55.561363Z"
    }
   },
   "outputs": [],
   "source": [
    "# We plot the empirical distribution of the test statistic (KS statistic) along with the observed statistic from the test which checked whether or not \"elders\" depended on \"patch\". I think that graphing is generally self explanatory.\n",
    "\n",
    "fig = px.histogram(\n",
    "    x=ks_stats,\n",
    "    nbins=50,\n",
    "    histnorm=\"probability\",\n",
    "    title=\"Empirical Distribution of KS ('elders' vs 'patch')\",\n",
    "    labels={\"x\": \"KS Statistic\", \"y\": \"Proportion\"}\n",
    ")\n",
    "\n",
    "fig.add_vline(x=observed_ks, line_color=\"red\", line_width=3)\n",
    "fig.add_annotation(\n",
    "    text=f\"Observed KS = {observed_ks:.3f}\",\n",
    "    x=observed_ks,\n",
    "    y=0.05,\n",
    "    showarrow=False,\n",
    "    font=dict(color=\"red\")\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Hypothesis Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T05:24:55.896025Z",
     "start_time": "2025-06-06T05:24:55.865042Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leona's win rate: 45.14%\n",
      "p: 0.0000038041\n",
      "Reject H0: Leona's win rate is significantly less than 50%.\n"
     ]
    }
   ],
   "source": [
    "# This filters the dataframe to only include entries for Leona, and then runs a binomial hypothesis test on it to determine whether or not Leona's win rate is significantly less than 50%, at a significance level of a = 0.05. This technique is mostly taken from lecture.\n",
    "\n",
    "# The null is H0: Leona’s win rate is not significantly less than 50%.\n",
    "# The alternative is H1: Leona’s win rate is significantly less than 50%.\n",
    "\n",
    "leona_df = df[df[\"champion\"] == \"Leona\"]\n",
    "\n",
    "leona_wins = leona_df[\"result\"].sum()\n",
    "leona_total = leona_df.shape[0]\n",
    "\n",
    "hypothesis_test = binomtest(k=leona_wins, n=leona_total, p=0.5, alternative='less')\n",
    "\n",
    "print(f\"Leona's win rate: {leona_wins / leona_total:.2%}\")\n",
    "print(f\"p: {hypothesis_test.pvalue:.10f}\")\n",
    "\n",
    "if hypothesis_test.pvalue < 0.05:\n",
    "    print(\"Reject H0: Leona's win rate is significantly less than 50%.\")\n",
    "else:\n",
    "    print(\"Fail to reject H0: No evidence Leona's win rate is less than 50%.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Framing a Prediction Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T05:24:55.901957Z",
     "start_time": "2025-06-06T05:24:55.897033Z"
    }
   },
   "outputs": [],
   "source": [
    "# We are, as suggested in Step 1, going to use the champion ban list to predict the first pick. If the ban list is itself not informative enough, we can take advantage of the additional features we have included (patch, teamname, position, player) to see if they make a difference and improve our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T05:24:55.924531Z",
     "start_time": "2025-06-06T05:24:55.902966Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ban1</th>\n",
       "      <th>ban2</th>\n",
       "      <th>ban3</th>\n",
       "      <th>ban4</th>\n",
       "      <th>...</th>\n",
       "      <th>ban7</th>\n",
       "      <th>ban8</th>\n",
       "      <th>ban9</th>\n",
       "      <th>ban10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Akali</td>\n",
       "      <td>Nocturne</td>\n",
       "      <td>K'Sante</td>\n",
       "      <td>Lee Sin</td>\n",
       "      <td>...</td>\n",
       "      <td>Ashe</td>\n",
       "      <td>Neeko</td>\n",
       "      <td>Vi</td>\n",
       "      <td>Jarvan IV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nocturne</td>\n",
       "      <td>Udyr</td>\n",
       "      <td>Renata Glasc</td>\n",
       "      <td>Nautilus</td>\n",
       "      <td>...</td>\n",
       "      <td>Ashe</td>\n",
       "      <td>Rumble</td>\n",
       "      <td>Tristana</td>\n",
       "      <td>Lucian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Rell</td>\n",
       "      <td>Nocturne</td>\n",
       "      <td>Tristana</td>\n",
       "      <td>Jarvan IV</td>\n",
       "      <td>...</td>\n",
       "      <td>Ashe</td>\n",
       "      <td>LeBlanc</td>\n",
       "      <td>Sejuani</td>\n",
       "      <td>Vi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Poppy</td>\n",
       "      <td>LeBlanc</td>\n",
       "      <td>Neeko</td>\n",
       "      <td>Sejuani</td>\n",
       "      <td>...</td>\n",
       "      <td>Nocturne</td>\n",
       "      <td>Ashe</td>\n",
       "      <td>Azir</td>\n",
       "      <td>Akali</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ashe</td>\n",
       "      <td>Akali</td>\n",
       "      <td>LeBlanc</td>\n",
       "      <td>Vi</td>\n",
       "      <td>...</td>\n",
       "      <td>Nocturne</td>\n",
       "      <td>Neeko</td>\n",
       "      <td>Sejuani</td>\n",
       "      <td>Poppy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8620</th>\n",
       "      <td>Sejuani</td>\n",
       "      <td>Vi</td>\n",
       "      <td>Ambessa</td>\n",
       "      <td>LeBlanc</td>\n",
       "      <td>...</td>\n",
       "      <td>Aurora</td>\n",
       "      <td>Skarner</td>\n",
       "      <td>Renekton</td>\n",
       "      <td>Maokai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8621</th>\n",
       "      <td>LeBlanc</td>\n",
       "      <td>Poppy</td>\n",
       "      <td>Azir</td>\n",
       "      <td>Bard</td>\n",
       "      <td>...</td>\n",
       "      <td>Skarner</td>\n",
       "      <td>Ambessa</td>\n",
       "      <td>Renekton</td>\n",
       "      <td>Rakan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8622</th>\n",
       "      <td>Renekton</td>\n",
       "      <td>Vi</td>\n",
       "      <td>K'Sante</td>\n",
       "      <td>Kalista</td>\n",
       "      <td>...</td>\n",
       "      <td>Skarner</td>\n",
       "      <td>Ambessa</td>\n",
       "      <td>Ashe</td>\n",
       "      <td>Varus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8623</th>\n",
       "      <td>LeBlanc</td>\n",
       "      <td>Poppy</td>\n",
       "      <td>Ambessa</td>\n",
       "      <td>Lee Sin</td>\n",
       "      <td>...</td>\n",
       "      <td>Nocturne</td>\n",
       "      <td>Azir</td>\n",
       "      <td>Gragas</td>\n",
       "      <td>Maokai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8624</th>\n",
       "      <td>Vi</td>\n",
       "      <td>Renekton</td>\n",
       "      <td>Skarner</td>\n",
       "      <td>Ashe</td>\n",
       "      <td>...</td>\n",
       "      <td>Poppy</td>\n",
       "      <td>Ambessa</td>\n",
       "      <td>Nocturne</td>\n",
       "      <td>Lee Sin</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8625 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ban1      ban2          ban3       ban4  ...      ban7     ban8  \\\n",
       "0        Akali  Nocturne       K'Sante    Lee Sin  ...      Ashe    Neeko   \n",
       "1     Nocturne      Udyr  Renata Glasc   Nautilus  ...      Ashe   Rumble   \n",
       "2         Rell  Nocturne      Tristana  Jarvan IV  ...      Ashe  LeBlanc   \n",
       "3        Poppy   LeBlanc         Neeko    Sejuani  ...  Nocturne     Ashe   \n",
       "4         Ashe     Akali       LeBlanc         Vi  ...  Nocturne    Neeko   \n",
       "...        ...       ...           ...        ...  ...       ...      ...   \n",
       "8620   Sejuani        Vi       Ambessa    LeBlanc  ...    Aurora  Skarner   \n",
       "8621   LeBlanc     Poppy          Azir       Bard  ...   Skarner  Ambessa   \n",
       "8622  Renekton        Vi       K'Sante    Kalista  ...   Skarner  Ambessa   \n",
       "8623   LeBlanc     Poppy       Ambessa    Lee Sin  ...  Nocturne     Azir   \n",
       "8624        Vi  Renekton       Skarner       Ashe  ...     Poppy  Ambessa   \n",
       "\n",
       "          ban9      ban10  \n",
       "0           Vi  Jarvan IV  \n",
       "1     Tristana     Lucian  \n",
       "2      Sejuani         Vi  \n",
       "3         Azir      Akali  \n",
       "4      Sejuani      Poppy  \n",
       "...        ...        ...  \n",
       "8620  Renekton     Maokai  \n",
       "8621  Renekton      Rakan  \n",
       "8622      Ashe      Varus  \n",
       "8623    Gragas     Maokai  \n",
       "8624  Nocturne    Lee Sin  \n",
       "\n",
       "[8625 rows x 15 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The cleaned and modified DataFrame we will be using, copied just in case.\n",
    "\n",
    "model_df = eval_df.copy()\n",
    "model_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T05:25:07.392985Z",
     "start_time": "2025-06-06T05:24:55.926046Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.19710144927536233\n"
     ]
    }
   ],
   "source": [
    "# The baseline model. It uses 10 features, columns ban1-ban10, to predict target pick1. We preprocess with a ColumnTransformer performing One-Hot Encoding and then insert it into the pipeline, which we then train with a train_test_split, and then fit. Accuracy is printed for analysis, using pipeline.score().\n",
    "\n",
    "features = [f\"ban{i}\" for i in range(1, 11)]\n",
    "\n",
    "banproc = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('transform', OneHotEncoder(handle_unknown=\"ignore\"), features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "pl = Pipeline([\n",
    "    ('preprocessor', banproc),\n",
    "    ('classifier', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "X = model_df[features]\n",
    "y = model_df[\"pick1\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=1)\n",
    "pl.fit(X_train, y_train)\n",
    "\n",
    "print(\"Accuracy:\", pl.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T05:30:13.914816Z",
     "start_time": "2025-06-06T05:25:07.395459Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 54 candidates, totalling 162 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mkbox\\Documents\\Anaconda\\Miniforge3\\envs\\dsc80\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:776: UserWarning:\n",
      "\n",
      "The least populated class in y has only 1 members, which is less than n_splits=3.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END classifier__max_depth=None, classifier__min_samples_leaf=1, classifier__min_samples_split=2, classifier__n_estimators=50; total time=   3.4s\n",
      "[CV] END classifier__max_depth=None, classifier__min_samples_leaf=1, classifier__min_samples_split=2, classifier__n_estimators=50; total time=   3.4s\n",
      "[CV] END classifier__max_depth=None, classifier__min_samples_leaf=1, classifier__min_samples_split=2, classifier__n_estimators=50; total time=   3.4s\n",
      "[CV] END classifier__max_depth=None, classifier__min_samples_leaf=1, classifier__min_samples_split=2, classifier__n_estimators=100; total time=   6.8s\n",
      "[CV] END classifier__max_depth=None, classifier__min_samples_leaf=1, classifier__min_samples_split=2, classifier__n_estimators=100; total time=   6.9s\n",
      "[CV] END classifier__max_depth=None, classifier__min_samples_leaf=1, classifier__min_samples_split=2, classifier__n_estimators=100; total time=   6.8s\n",
      "[CV] END classifier__max_depth=None, classifier__min_samples_leaf=1, classifier__min_samples_split=2, classifier__n_estimators=200; total time=  13.6s\n",
      "[CV] END classifier__max_depth=None, classifier__min_samples_leaf=1, classifier__min_samples_split=2, classifier__n_estimators=200; total time=  13.6s\n",
      "[CV] END classifier__max_depth=None, classifier__min_samples_leaf=1, classifier__min_samples_split=2, classifier__n_estimators=200; total time=  19.1s\n",
      "[CV] END classifier__max_depth=None, classifier__min_samples_leaf=1, classifier__min_samples_split=5, classifier__n_estimators=50; total time=   3.8s\n",
      "[CV] END classifier__max_depth=None, classifier__min_samples_leaf=1, classifier__min_samples_split=5, classifier__n_estimators=50; total time=   3.6s\n",
      "[CV] END classifier__max_depth=None, classifier__min_samples_leaf=1, classifier__min_samples_split=5, classifier__n_estimators=50; total time=   3.7s\n",
      "[CV] END classifier__max_depth=None, classifier__min_samples_leaf=1, classifier__min_samples_split=5, classifier__n_estimators=100; total time=   7.4s\n",
      "[CV] END classifier__max_depth=None, classifier__min_samples_leaf=1, classifier__min_samples_split=5, classifier__n_estimators=100; total time=   7.5s\n",
      "[CV] END classifier__max_depth=None, classifier__min_samples_leaf=1, classifier__min_samples_split=5, classifier__n_estimators=100; total time=   7.6s\n",
      "[CV] END classifier__max_depth=None, classifier__min_samples_leaf=1, classifier__min_samples_split=5, classifier__n_estimators=200; total time=  10.7s\n",
      "[CV] END classifier__max_depth=None, classifier__min_samples_leaf=1, classifier__min_samples_split=5, classifier__n_estimators=200; total time=   8.2s\n",
      "[CV] END classifier__max_depth=None, classifier__min_samples_leaf=1, classifier__min_samples_split=5, classifier__n_estimators=200; total time=   8.3s\n",
      "[CV] END classifier__max_depth=None, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=50; total time=   1.0s\n",
      "[CV] END classifier__max_depth=None, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=50; total time=   0.9s\n",
      "[CV] END classifier__max_depth=None, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=50; total time=   1.0s\n",
      "[CV] END classifier__max_depth=None, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=100; total time=   2.0s\n",
      "[CV] END classifier__max_depth=None, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=100; total time=   2.0s\n",
      "[CV] END classifier__max_depth=None, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=100; total time=   2.0s\n",
      "[CV] END classifier__max_depth=None, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=200; total time=   4.0s\n",
      "[CV] END classifier__max_depth=None, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=200; total time=   4.0s\n",
      "[CV] END classifier__max_depth=None, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=200; total time=   4.0s\n",
      "[CV] END classifier__max_depth=None, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=50; total time=   0.9s\n",
      "[CV] END classifier__max_depth=None, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=50; total time=   0.9s\n",
      "[CV] END classifier__max_depth=None, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=50; total time=   0.9s\n",
      "[CV] END classifier__max_depth=None, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=100; total time=   1.9s\n",
      "[CV] END classifier__max_depth=None, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=100; total time=   1.9s\n",
      "[CV] END classifier__max_depth=None, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=100; total time=   1.9s\n",
      "[CV] END classifier__max_depth=None, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=200; total time=   3.8s\n",
      "[CV] END classifier__max_depth=None, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=200; total time=   3.8s\n",
      "[CV] END classifier__max_depth=None, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=200; total time=   3.8s\n",
      "[CV] END classifier__max_depth=None, classifier__min_samples_leaf=3, classifier__min_samples_split=2, classifier__n_estimators=50; total time=   0.6s\n",
      "[CV] END classifier__max_depth=None, classifier__min_samples_leaf=3, classifier__min_samples_split=2, classifier__n_estimators=50; total time=   0.6s\n",
      "[CV] END classifier__max_depth=None, classifier__min_samples_leaf=3, classifier__min_samples_split=2, classifier__n_estimators=50; total time=   0.6s\n",
      "[CV] END classifier__max_depth=None, classifier__min_samples_leaf=3, classifier__min_samples_split=2, classifier__n_estimators=100; total time=   1.3s\n",
      "[CV] END classifier__max_depth=None, classifier__min_samples_leaf=3, classifier__min_samples_split=2, classifier__n_estimators=100; total time=   1.3s\n",
      "[CV] END classifier__max_depth=None, classifier__min_samples_leaf=3, classifier__min_samples_split=2, classifier__n_estimators=100; total time=   1.3s\n",
      "[CV] END classifier__max_depth=None, classifier__min_samples_leaf=3, classifier__min_samples_split=2, classifier__n_estimators=200; total time=   2.6s\n",
      "[CV] END classifier__max_depth=None, classifier__min_samples_leaf=3, classifier__min_samples_split=2, classifier__n_estimators=200; total time=   2.6s\n",
      "[CV] END classifier__max_depth=None, classifier__min_samples_leaf=3, classifier__min_samples_split=2, classifier__n_estimators=200; total time=   2.6s\n",
      "[CV] END classifier__max_depth=None, classifier__min_samples_leaf=3, classifier__min_samples_split=5, classifier__n_estimators=50; total time=   0.6s\n",
      "[CV] END classifier__max_depth=None, classifier__min_samples_leaf=3, classifier__min_samples_split=5, classifier__n_estimators=50; total time=   0.6s\n",
      "[CV] END classifier__max_depth=None, classifier__min_samples_leaf=3, classifier__min_samples_split=5, classifier__n_estimators=50; total time=   0.6s\n",
      "[CV] END classifier__max_depth=None, classifier__min_samples_leaf=3, classifier__min_samples_split=5, classifier__n_estimators=100; total time=   1.3s\n",
      "[CV] END classifier__max_depth=None, classifier__min_samples_leaf=3, classifier__min_samples_split=5, classifier__n_estimators=100; total time=   1.3s\n",
      "[CV] END classifier__max_depth=None, classifier__min_samples_leaf=3, classifier__min_samples_split=5, classifier__n_estimators=100; total time=   1.3s\n",
      "[CV] END classifier__max_depth=None, classifier__min_samples_leaf=3, classifier__min_samples_split=5, classifier__n_estimators=200; total time=   2.6s\n",
      "[CV] END classifier__max_depth=None, classifier__min_samples_leaf=3, classifier__min_samples_split=5, classifier__n_estimators=200; total time=   2.6s\n",
      "[CV] END classifier__max_depth=None, classifier__min_samples_leaf=3, classifier__min_samples_split=5, classifier__n_estimators=200; total time=   2.6s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_leaf=1, classifier__min_samples_split=2, classifier__n_estimators=50; total time=   0.2s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_leaf=1, classifier__min_samples_split=2, classifier__n_estimators=50; total time=   0.2s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_leaf=1, classifier__min_samples_split=2, classifier__n_estimators=50; total time=   0.2s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_leaf=1, classifier__min_samples_split=2, classifier__n_estimators=100; total time=   0.5s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_leaf=1, classifier__min_samples_split=2, classifier__n_estimators=100; total time=   0.4s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_leaf=1, classifier__min_samples_split=2, classifier__n_estimators=100; total time=   0.5s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_leaf=1, classifier__min_samples_split=2, classifier__n_estimators=200; total time=   1.0s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_leaf=1, classifier__min_samples_split=2, classifier__n_estimators=200; total time=   1.0s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_leaf=1, classifier__min_samples_split=2, classifier__n_estimators=200; total time=   0.9s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_leaf=1, classifier__min_samples_split=5, classifier__n_estimators=50; total time=   0.2s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_leaf=1, classifier__min_samples_split=5, classifier__n_estimators=50; total time=   0.2s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_leaf=1, classifier__min_samples_split=5, classifier__n_estimators=50; total time=   0.2s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_leaf=1, classifier__min_samples_split=5, classifier__n_estimators=100; total time=   0.4s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_leaf=1, classifier__min_samples_split=5, classifier__n_estimators=100; total time=   0.4s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_leaf=1, classifier__min_samples_split=5, classifier__n_estimators=100; total time=   0.4s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_leaf=1, classifier__min_samples_split=5, classifier__n_estimators=200; total time=   0.9s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_leaf=1, classifier__min_samples_split=5, classifier__n_estimators=200; total time=   0.9s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_leaf=1, classifier__min_samples_split=5, classifier__n_estimators=200; total time=   0.8s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=50; total time=   0.2s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=50; total time=   0.2s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=50; total time=   0.2s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=100; total time=   0.4s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=100; total time=   0.4s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=100; total time=   0.4s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=200; total time=   0.8s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=200; total time=   0.8s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=200; total time=   0.8s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=50; total time=   0.2s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=50; total time=   0.2s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=50; total time=   0.2s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=100; total time=   0.4s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=100; total time=   0.4s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=100; total time=   0.4s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=200; total time=   0.8s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=200; total time=   0.8s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=200; total time=   0.8s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_leaf=3, classifier__min_samples_split=2, classifier__n_estimators=50; total time=   0.2s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_leaf=3, classifier__min_samples_split=2, classifier__n_estimators=50; total time=   0.2s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_leaf=3, classifier__min_samples_split=2, classifier__n_estimators=50; total time=   0.1s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_leaf=3, classifier__min_samples_split=2, classifier__n_estimators=100; total time=   0.4s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_leaf=3, classifier__min_samples_split=2, classifier__n_estimators=100; total time=   0.4s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_leaf=3, classifier__min_samples_split=2, classifier__n_estimators=100; total time=   0.4s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_leaf=3, classifier__min_samples_split=2, classifier__n_estimators=200; total time=   0.8s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_leaf=3, classifier__min_samples_split=2, classifier__n_estimators=200; total time=   0.8s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_leaf=3, classifier__min_samples_split=2, classifier__n_estimators=200; total time=   0.8s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_leaf=3, classifier__min_samples_split=5, classifier__n_estimators=50; total time=   0.2s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_leaf=3, classifier__min_samples_split=5, classifier__n_estimators=50; total time=   0.1s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_leaf=3, classifier__min_samples_split=5, classifier__n_estimators=50; total time=   0.2s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_leaf=3, classifier__min_samples_split=5, classifier__n_estimators=100; total time=   0.4s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_leaf=3, classifier__min_samples_split=5, classifier__n_estimators=100; total time=   0.4s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_leaf=3, classifier__min_samples_split=5, classifier__n_estimators=100; total time=   0.4s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_leaf=3, classifier__min_samples_split=5, classifier__n_estimators=200; total time=   0.8s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_leaf=3, classifier__min_samples_split=5, classifier__n_estimators=200; total time=   0.8s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_leaf=3, classifier__min_samples_split=5, classifier__n_estimators=200; total time=   0.8s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_leaf=1, classifier__min_samples_split=2, classifier__n_estimators=50; total time=   0.5s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_leaf=1, classifier__min_samples_split=2, classifier__n_estimators=50; total time=   0.5s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_leaf=1, classifier__min_samples_split=2, classifier__n_estimators=50; total time=   0.5s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_leaf=1, classifier__min_samples_split=2, classifier__n_estimators=100; total time=   1.0s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_leaf=1, classifier__min_samples_split=2, classifier__n_estimators=100; total time=   1.0s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_leaf=1, classifier__min_samples_split=2, classifier__n_estimators=100; total time=   1.0s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_leaf=1, classifier__min_samples_split=2, classifier__n_estimators=200; total time=   2.1s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_leaf=1, classifier__min_samples_split=2, classifier__n_estimators=200; total time=   2.1s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_leaf=1, classifier__min_samples_split=2, classifier__n_estimators=200; total time=   2.1s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_leaf=1, classifier__min_samples_split=5, classifier__n_estimators=50; total time=   0.4s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_leaf=1, classifier__min_samples_split=5, classifier__n_estimators=50; total time=   0.4s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_leaf=1, classifier__min_samples_split=5, classifier__n_estimators=50; total time=   0.4s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_leaf=1, classifier__min_samples_split=5, classifier__n_estimators=100; total time=   0.8s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_leaf=1, classifier__min_samples_split=5, classifier__n_estimators=100; total time=   0.8s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_leaf=1, classifier__min_samples_split=5, classifier__n_estimators=100; total time=   0.8s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_leaf=1, classifier__min_samples_split=5, classifier__n_estimators=200; total time=   1.6s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_leaf=1, classifier__min_samples_split=5, classifier__n_estimators=200; total time=   1.6s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_leaf=1, classifier__min_samples_split=5, classifier__n_estimators=200; total time=   1.6s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=50; total time=   0.3s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=50; total time=   0.3s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=50; total time=   0.3s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=100; total time=   0.6s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=100; total time=   0.7s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=100; total time=   0.7s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=200; total time=   1.3s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=200; total time=   1.4s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=200; total time=   1.3s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=50; total time=   0.3s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=50; total time=   0.3s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=50; total time=   0.3s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=100; total time=   0.6s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=100; total time=   0.6s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=100; total time=   0.6s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=200; total time=   1.3s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=200; total time=   1.3s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=200; total time=   1.3s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_leaf=3, classifier__min_samples_split=2, classifier__n_estimators=50; total time=   0.2s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_leaf=3, classifier__min_samples_split=2, classifier__n_estimators=50; total time=   0.2s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_leaf=3, classifier__min_samples_split=2, classifier__n_estimators=50; total time=   0.2s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_leaf=3, classifier__min_samples_split=2, classifier__n_estimators=100; total time=   0.6s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_leaf=3, classifier__min_samples_split=2, classifier__n_estimators=100; total time=   0.5s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_leaf=3, classifier__min_samples_split=2, classifier__n_estimators=100; total time=   0.6s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_leaf=3, classifier__min_samples_split=2, classifier__n_estimators=200; total time=   1.2s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_leaf=3, classifier__min_samples_split=2, classifier__n_estimators=200; total time=   1.2s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_leaf=3, classifier__min_samples_split=2, classifier__n_estimators=200; total time=   1.1s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_leaf=3, classifier__min_samples_split=5, classifier__n_estimators=50; total time=   0.2s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_leaf=3, classifier__min_samples_split=5, classifier__n_estimators=50; total time=   0.2s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_leaf=3, classifier__min_samples_split=5, classifier__n_estimators=50; total time=   0.2s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_leaf=3, classifier__min_samples_split=5, classifier__n_estimators=100; total time=   0.5s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_leaf=3, classifier__min_samples_split=5, classifier__n_estimators=100; total time=   0.5s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_leaf=3, classifier__min_samples_split=5, classifier__n_estimators=100; total time=   0.5s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_leaf=3, classifier__min_samples_split=5, classifier__n_estimators=200; total time=   1.2s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_leaf=3, classifier__min_samples_split=5, classifier__n_estimators=200; total time=   1.1s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_leaf=3, classifier__min_samples_split=5, classifier__n_estimators=200; total time=   1.2s\n",
      "RandomForestClassifier Parameters: {'classifier__max_depth': None, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 5, 'classifier__n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "# I chose to test these hyperparameters because I thought that if there was any one issue with my model, it might be overfitting. All of these hyperparameters one way or another govern how fit the model is, so I thought they would be good ideas. I list the specific reasons for the ones I chose below.\n",
    "\n",
    "# n_estimators, the number of trees in the forest, because I felt that increasing them might improve accuracy.\n",
    "# max_depth, the maximum depth of each tree, because I was worried about overfitting.\n",
    "# min_samples_leaf, the minimum samples on each leaf, because I was worried about overfitting.\n",
    "# min_samples_split, the minimum number of samples required to split a node, because I was worried about overfitting.\n",
    "\n",
    "# All testing was done with GridSearchCV.\n",
    "\n",
    "hyperparameters = {\n",
    "    'classifier__n_estimators': [50, 100, 200],\n",
    "    'classifier__max_depth': [None, 10, 20],\n",
    "    'classifier__min_samples_split': [2, 5],\n",
    "    'classifier__min_samples_leaf': [1, 2, 3],\n",
    "}\n",
    "\n",
    "rf_search = GridSearchCV(pl, hyperparameters, cv=3, verbose=2)\n",
    "rf_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"RandomForestClassifier Parameters:\", rf_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T05:30:29.748149Z",
     "start_time": "2025-06-06T05:30:13.916294Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4736231884057971\n"
     ]
    }
   ],
   "source": [
    "# The final model. The three new features encoded here are position, playername, and patch, which are all One-Hot Encoded along with the list of bans 1-10. This is a total of 13 features. We preprocess with a ColumnTransformer performing One-Hot Encoding and then insert it into the pipeline, which we then train with a train_test_split, and then fit. Accuracy is printed for analysis, using pipeline.score().\n",
    "\n",
    "features = [f\"ban{i}\" for i in range(1, 11)] + [\"position\", \"playername\", \"patch\"]\n",
    "\n",
    "banproc = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('transform', OneHotEncoder(handle_unknown=\"ignore\"), features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "pl = Pipeline([\n",
    "    ('preprocessor', banproc),\n",
    "    ('classifier', RandomForestClassifier(max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=200))\n",
    "])\n",
    "\n",
    "X = model_df[features]\n",
    "y = model_df[\"pick1\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=1)\n",
    "pl.fit(X_train, y_train)\n",
    "\n",
    "print(\"Accuracy:\", pl.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Fairness Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T05:30:43.524843Z",
     "start_time": "2025-06-06T05:30:29.750163Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p: 0.1860000000\n",
      "Fail to reject H0: Our model is fair. Its precision for bot lane is the same as its precision for mid lane.\n"
     ]
    }
   ],
   "source": [
    "# For my fairness analysis, I'm going to check precision. I want to see if I guess champions for one of two heavily played positions more than another. Those positions are bot and mid, which are also decently similar in presence.\n",
    "\n",
    "# Null hypothesis H0: Our model is fair. Its precision for bot lane is the same as its precision for mid lane.\n",
    "# Alternative hypothesis H1: Our model is unfair. Its precision for bot lane is not the same as its precision for mid lane.\n",
    "\n",
    "# To accomplish this, I effectively reuse the permutation test code from Step 3 with a little extra work.\n",
    "\n",
    "n_permutations = 1000\n",
    "\n",
    "position_df = X_test.copy()\n",
    "position_df[\"y_actual\"] = y_test.copy()\n",
    "position_df[\"y_predicted\"] = pl.predict(X_test)\n",
    "position_df = position_df[position_df[\"position\"].isin([\"bot\", \"mid\"])]\n",
    "\n",
    "precision_bot = precision_score(\n",
    "    position_df[position_df[\"position\"] == \"bot\"][\"y_actual\"],\n",
    "    position_df[position_df[\"position\"] == \"bot\"][\"y_predicted\"],\n",
    "    average=\"weighted\",\n",
    "    zero_division=0\n",
    ")\n",
    "\n",
    "precision_mid = precision_score(\n",
    "    position_df[position_df[\"position\"] == \"mid\"][\"y_actual\"],\n",
    "    position_df[position_df[\"position\"] == \"mid\"][\"y_predicted\"],\n",
    "    average=\"weighted\",\n",
    "    zero_division=0\n",
    ")\n",
    "\n",
    "# observed value\n",
    "observed_abs_diff = abs(precision_bot - precision_mid)\n",
    "\n",
    "# here is the permutation test itself, looping the observed value code\n",
    "abs_diffs = []\n",
    "for _ in range(n_permutations):\n",
    "    shuffled = np.random.permutation(position_df[\"position\"])\n",
    "    shuffled_df = position_df.copy()\n",
    "    shuffled_df[\"shuffled_position\"] = shuffled\n",
    "\n",
    "    shuffled_bot = precision_score(\n",
    "        shuffled_df[shuffled_df[\"shuffled_position\"] == \"bot\"][\"y_actual\"],\n",
    "        shuffled_df[shuffled_df[\"shuffled_position\"] == \"bot\"][\"y_predicted\"],\n",
    "        average=\"weighted\",\n",
    "        zero_division=0\n",
    "    )\n",
    "\n",
    "    shuffled_mid = precision_score(\n",
    "        shuffled_df[shuffled_df[\"shuffled_position\"] == \"mid\"][\"y_actual\"],\n",
    "        shuffled_df[shuffled_df[\"shuffled_position\"] == \"mid\"][\"y_predicted\"],\n",
    "        average=\"weighted\",\n",
    "        zero_division=0\n",
    "    )\n",
    "    \n",
    "    abs_diff = abs(shuffled_bot - shuffled_mid)\n",
    "    abs_diffs.append(abs_diff)\n",
    "\n",
    "# p-value\n",
    "p = np.mean(np.array(abs_diffs) >= observed_abs_diff)\n",
    "\n",
    "print(f\"p: {p:.10f}\")\n",
    "\n",
    "if p < 0.05:\n",
    "    print(\"Reject H0: Our model is unfair. Its precision for bot lane is not the same as its precision for mid lane.\")\n",
    "else:\n",
    "    print(\"Fail to reject H0: Our model is fair. Its precision for bot lane is the same as its precision for mid lane.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
